{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71838751",
   "metadata": {},
   "source": [
    "Use the following google drive link incase code is not running in your local envionment\n",
    "https://drive.google.com/file/d/10WFbJYJ5-bGeeItmgGEFkin_Iy95jgP3/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4735c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0453d72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocessing():\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        self.name = filename\n",
    "        self.df = pd.read_csv(filename+\".csv\")\n",
    "    \n",
    "    def get_dataframe(self):\n",
    "        return self.df\n",
    "\n",
    "    def display(self):\n",
    "        display(self.df.head())\n",
    "        \n",
    "    #Removing unicode characters (eg: //u2fed)\n",
    "    def remove_unicode(self):\n",
    "        count = 0\n",
    "        string = ''\n",
    "        for index,line in enumerate(self.df['text']):\n",
    "            if '\\\\u' in line:\n",
    "                line = line.replace('\\\\u',' #')\n",
    "                token = line.split()\n",
    "                count+=1\n",
    "                for word in token:\n",
    "                    if '#' not in word:\n",
    "                        string += word + ' '\n",
    "                self.df['text'][index] = string\n",
    "                string = ''\n",
    "        print(\"_____________________\",\"Removed Unicode\",\"________________________\")\n",
    "                \n",
    "    #Converting to Lowercase\n",
    "    def lowercase(self):\n",
    "        self.df['text'] = self.df['text'].str.lower()\n",
    "        print(\"_____________________\",\"All text are lowercased\",\"________________________\")\n",
    "        \n",
    "    #Negation Words\n",
    "    def negation_words(self):\n",
    "        negated_dictionary = {'don\\'t':\"do not\",'didn\\'t': \"did not\",'can\\'t':\"cannot\",'wouldn\\'t':\"would not\",'doesn\\'t':\"does not\",'i\\'m':'i am'}\n",
    "        #confusing_dictionary = {'vote out':'loser','out of office':'loser','vote him out':'loser'}\n",
    "        for index,comments in enumerate(self.df['text']):\n",
    "            text=\"\"\n",
    "            py_stem = PorterStemmer()\n",
    "            token = comments.split()\n",
    "            for word_index,word in enumerate(token):\n",
    "                if word in negated_dictionary.keys():\n",
    "                    token[word_index] = negated_dictionary[word]\n",
    "                stemmed_word = py_stem.stem(word)\n",
    "                if word_index+2 < len(token) and stemmed_word == \"vote\" and token[word_index+1] == \"him\" and token[word_index+2] == \"out\":\n",
    "                    token.remove(\"him\")\n",
    "                    token.remove(\"out\")\n",
    "                    token[word_index] = \"loser\"\n",
    "                if word_index+2 < len(token) and token[word_index] == \"out\" and token[word_index+1] == \"of\" and token[word_index+2] == \"office\":\n",
    "                    token.remove(\"of\")\n",
    "                    token.remove(\"office\")\n",
    "                    token[word_index] = \"loser\"\n",
    "                if word_index+1 < len(token)and stemmed_word == \"vote\" and token[word_index+1] == \"out\":\n",
    "                    token.remove(\"out\")\n",
    "                    token[word_index] = \"loser\"\n",
    "                text = \" \".join(token)\n",
    "            self.df['text'][index] = text\n",
    "        print(\"_____________________\",\"Expanded Negated words\",\"________________________\")\n",
    "        \n",
    "    #Applying Filter Rules\n",
    "    def filter_rules(self):\n",
    "\n",
    "        for index,line in enumerate(self.df['text']):\n",
    "            line = line.replace(\"\\\\n\",\"\") # Replacing New Lines\n",
    "            line = re.sub('http://\\S+|https://\\S+', '', line)  #Replacing URLS\n",
    "            #line = re.sub('\\S*@\\S*\\s?','',line)   #Replacing emails\n",
    "            line = re.sub('\\([^)]*\\)','',line) #Replacing (strings)\n",
    "            #line = re.sub('[^a-z]+',' ',line) #Replacing alphanumeric characters\n",
    "            self.df['text'][index] = line\n",
    "            \n",
    "        print(\"_____________________\",\"Applied Filter rules\",\"________________________\")\n",
    "    \n",
    "    def save_csv(self,string):\n",
    "        self.df.to_csv(\"Output/{}_{}.csv\".format(string,self.name[-3:]))\n",
    "        print(\"_____________________\",\"File saved\",\"________________________\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e297d773",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350c25f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = preprocessing(\"Data/Forecasting_CNN\")\n",
    "#obj.display()\n",
    "obj.remove_unicode()\n",
    "obj.lowercase()\n",
    "obj.negation_words()\n",
    "obj.filter_rules\n",
    "obj.save_csv(\"preprocessing\") #Parameter: Give a name to save"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
