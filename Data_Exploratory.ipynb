{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4de7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "sw_nltk = stopwords.words('english')\n",
    "import plotly.express as px\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec895ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_exploratory:\n",
    "    \n",
    "    def __init__(self, filename):\n",
    "        self.name = filename\n",
    "        self.df = pd.read_csv(filename+\".csv\")\n",
    "        self.pre_df = pd.DataFrame()\n",
    "        self.post_df = pd.DataFrame()\n",
    "    \n",
    "    def get_dataframe(self):\n",
    "        return self.df\n",
    "    \n",
    "    def display(self):\n",
    "        display(self.df.head())\n",
    "        \n",
    "    #Calculating the avg, highest, lowest length from the entire corpus\n",
    "    def count_words(self, df):\n",
    "        highest_word_count = 0\n",
    "        lowest_word_count = 1000\n",
    "        length = 0\n",
    "        total = 0\n",
    "        doc_length = []\n",
    "        for index,line in enumerate(df['text']):\n",
    "            length = len(line)\n",
    "            if length > highest_word_count:\n",
    "                highest_word_count = length\n",
    "            elif length < lowest_word_count:\n",
    "                lowest_word_count = length\n",
    "            total+=length\n",
    "            doc_length.append(length)\n",
    "        print(\"Highest Word Count:\",highest_word_count)\n",
    "        print(\"Lowest Word Count:\",lowest_word_count)\n",
    "        print(\"Average Word Count:\",total/len(self.df))\n",
    "        \n",
    "    \n",
    "    def bigram_trigram(self, df):\n",
    "        bigram_list = []\n",
    "        doc = []\n",
    "        py_stem = PorterStemmer ()\n",
    "        for comments in df['text']:\n",
    "            tokens = [py_stem.stem(word) for word in comments.split()]\n",
    "            for w in tokens: \n",
    "                if not w in sw_nltk:\n",
    "                    doc.append(w)\n",
    "        # get bigrams\n",
    "        _2gram = ['_'.join(e) for e in ngrams(doc, 2)]\n",
    "        _3gram = ['_'.join(e) for e in ngrams(doc, 3)]\n",
    "        bigram_count = Counter(_3gram)\n",
    "        print(bigram_count.most_common(20))\n",
    "    \n",
    "    #Function to count the keywords from the entire corpus\n",
    "    def count_keywords(self,df):\n",
    "\n",
    "        freq_df = df.copy()\n",
    "        freq_df['text'] = freq_df['text'].str.lower()\n",
    "        text = ' '.join(freq_df.text)\n",
    "        py_stem = PorterStemmer ()\n",
    "        token = [py_stem.stem(word) for word in text.split()]\n",
    "        keywords = ['joe', 'biden', 'donald', 'won', 'trump', 'winner', 'win', 'vote','donaldtrump','joebiden']\n",
    "        freq = {}\n",
    "        for item in keywords:\n",
    "            freq[item] = token.count(item)\n",
    "        freq_df = pd.DataFrame(list(freq.items()), columns = ['keywords','count'])\n",
    "        print(freq_df)\n",
    "        fig = px.bar(freq_df, x='keywords', y='count')\n",
    "        fig.show('notebook')\n",
    "        \n",
    "    \n",
    "    #Filtering 15 days of data before the election day (2020-10-19 - 2020-11-02)\n",
    "    def pre_election(self,count_bool = False):\n",
    "        self.pre_df = self.df[self.df['timeline'] < '2020-11-03']\n",
    "        if count_bool:\n",
    "            self.count_keywords(self.pre_df)\n",
    "            self.count_words(self.pre_df)\n",
    "\n",
    "    #Filtering 5 days from the election date (2020-11-03 - 2020-11-07)\n",
    "    def post_election(self, count_bool = False):\n",
    "        self.post_df = self.df[self.df['timeline'] >= '2020-11-03']\n",
    "        if count_bool:\n",
    "            self.count_keywords(self.post_df)\n",
    "            self.count_words(self.post_df)\n",
    "            \n",
    "    # Document is trimmed according to the data exploratory analysis to plus or minus k words \n",
    "    # around anchor words (eg:-20 words,'vote', +20 words)\n",
    "    def trim_document(self,n):\n",
    "        \n",
    "        for index,value in enumerate(self.pre_df['text']):\n",
    "            minus_string = ''\n",
    "            positive_string = ''\n",
    "            value = word_tokenize(value)\n",
    "            #print(len(value))\n",
    "            #print(value)\n",
    "            if len(value) > n*2:\n",
    "                py_stem = PorterStemmer ()\n",
    "                token = [py_stem.stem(word) for word in value]\n",
    "                token_index = 0\n",
    "                for word in token:\n",
    "                    if word in ['vote','win','won','winner']:\n",
    "                        token_index = token.index(word)\n",
    "                    if token_index <=n:\n",
    "                        minus_ngram = value[:token_index]\n",
    "                    else:\n",
    "                        minus_ngram = value[token_index-n:token_index]\n",
    "                    if len(token) > token_index+n:\n",
    "                        positive_ngram = value[token_index:token_index+n]\n",
    "                    else:\n",
    "                        positive_ngram = value[token_index:len(token)]\n",
    "                minus_string = ' '.join(minus_ngram)\n",
    "                positive_string = ' '.join(positive_ngram)\n",
    "                self.pre_df['text'][index] = minus_string + \" \"+ positive_string\n",
    "        self.pre_df.to_csv(\"Output/trimmed_pre_election_punctuation_CNN_{}.csv\".format(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39110158",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = data_exploratory(\"Output/preprocessing_CNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c758a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.count_words(obj.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26601efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.bigram_trigram(obj.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9562bd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.pre_election(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f406ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "obj.post_election(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07209b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.trim_document(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
